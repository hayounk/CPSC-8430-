{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "19oOHx4rcjKZPCyrPNXAEhF2rK8FRUHmj",
      "authorship_tag": "ABX9TyPRatk6NcPrCRoNU5AFiz7O"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxbUTfyZm8UX"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "from scipy import linalg\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg_9ir4YIzS6"
      },
      "source": [
        "workers = 2\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "image_size = 64\n",
        "\n",
        "nc = 3\n",
        "nz = 100\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "lr = 0.0002\n",
        "\n",
        "beta1 = 0.5\n",
        "\n",
        "ngpu = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coqLs7c3I6yH"
      },
      "source": [
        "dataroot = \"data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RX4tDoBJSsW"
      },
      "source": [
        "def weights_init(m) :\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1 :\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1 :\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tmgIfFJJUoe"
      },
      "source": [
        "class Generator(nn.Module) :\n",
        "    def __init__(self, ngpu) :\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input) :\n",
        "        return self.main(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEgJdjxvMWkp"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.Scale(64),\n",
        "        transforms.ToTensor(),                     \n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = dsets.CIFAR10(root='./data/', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size= batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA_SoudGQEmD"
      },
      "source": [
        "class Discriminator(nn.Module) :\n",
        "    def __init__(self, ngpu) :\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input) :\n",
        "        return self.main(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKrIG8EYSKTT"
      },
      "source": [
        "netG = Generator(ngpu)\n",
        "#if (device.type=='cuda') and (ngpu>1):\n",
        "#   netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "netG.apply(weights_init) \n",
        "print(netG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9C-Gu9ySfrO"
      },
      "source": [
        "netD = Discriminator(ngpu) S\n",
        "\n",
        "netD.apply(weights_init) \n",
        "print(netD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg33BllpQlxF"
      },
      "source": [
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "fixed_noise = torch.randn(64, nz, 1, 1)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zXMCQfir0vk"
      },
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False):\n",
        "        \n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        \n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "         # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "             # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "    \n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "model = InceptionV3([block_idx])\n",
        "model=model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JpUV4ZLsA5T"
      },
      "source": [
        "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
        "                    cuda=False):\n",
        "    model.eval()\n",
        "    act=np.empty((len(images), dims))\n",
        "    \n",
        "    if cuda:\n",
        "        batch=images.cuda()\n",
        "    else:\n",
        "        batch=images\n",
        "    pred = model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
        "    \n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOsPuYQ1sC4n"
      },
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    \n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all() :\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    if np.iscomplexobj(covmean) :\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDQ6LXyksFwS"
      },
      "source": [
        "def calculate_fretchet(images_real,images_fake,model):\n",
        "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
        "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
        "    \n",
        "     \"\"\"get fretched distance\"\"\"\n",
        "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
        "     return fid_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTPPI89DsGWh"
      },
      "source": [
        "print(\"Generator Parameters:\",sum(p.numel() for p in netG.parameters() if p.requires_grad))\n",
        "print(\"Discriminator Parameters:\",sum(p.numel() for p in netD.parameters() if p.requires_grad))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzeyP5T_Qt3X"
      },
      "source": [
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(train_loader, 0) :\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0]\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float)\n",
        "        \n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        \n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        \n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        # D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1)\n",
        "        \n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        \n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        \n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        \n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        # D_G_z1 = output.mean().item()\n",
        "        \n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        \n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        \n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        \n",
        "        output = netD(fake).view(-1)\n",
        "        \n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        \n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        \n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "         # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(train_loader)-1)) :\n",
        "            with torch.no_grad() :\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "        iters += 1        \n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "        fretchet_dist=calculate_fretchet(real_cpu,fake,model)\n",
        "\n",
        "       # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\ FID: %.4f'\n",
        "                  % (epoch + 1, num_epochs,\n",
        "                     errD.item(), errG.item(), fretchet_dist))\n",
        "        \n",
        "            plt.figure(figsize=(8,8))\n",
        "            plt.axis(\"off\")\n",
        "            pictures=vutils.make_grid(fake[torch.randint(len(fake), (10,))],nrow=5,padding=2, normalize=True)\n",
        "            plt.imshow(np.transpose(pictures,(1,2,0)))\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TsLWd9L6Ce-"
      },
      "source": [
        "plt.figure(figsize=(10,5)) \n",
        "plt.title(\"Generator and Discriminator Loss During Training\") \n",
        "plt.plot(G_losses,label=\"G\") \n",
        "plt.plot(D_losses,label=\"D\") \n",
        "plt.xlabel(\"iterations\") \n",
        "plt.ylabel(\"Loss\") \n",
        "plt.legend() \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK_9FQ6d6cCk"
      },
      "source": [
        "# Grab a batch of real images from the dataloader\n",
        "real_batch = next(iter(train_loader))\n",
        "\n",
        "# Plot the real images\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "# Plot the fake images from the last epoch\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fake Images\")\n",
        "pictures = vutils.make_grid()\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}